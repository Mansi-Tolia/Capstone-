---
title: "Untitled"
author: "Lyufan Pan"
date: "March 19, 2020"
output: html_document
---

# Student Loan Default Rate

-----

Load those libraries!!
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table)
library(corrplot)
library(factoextra)
library(readr)
library(matrixStats)
library(randomForest)
library(DescTools)
library(skimr)
```

# Data cleaning
```{r message=FALSE, warning=FALSE}
#load
scorecard <- readRDS("~/Desktop/cleaned_scorecard.rds")
ipeds <- read_csv("~/Desktop/ipeds.csv")

#basic clean
names(scorecard) <- tolower(names(scorecard))
names(ipeds) <- tolower(names(ipeds))

#merge datasets based on unitid and year
combined <- merge(ipeds, scorecard, by = c("unitid", "year"))

#separate out ACT/SAT for late evalutation
testScores <- combined %>% select(unitid, year, starts_with("act"), starts_with("sat"))

#separate out repayment rate
repaymentRate <- combined %>% select(unitid, year, contains("rpy"))

#combine default rates
combined$default <- rowMeans(combined[,c("cdr2","cdr3")], na.rm = T)

#remove empty columns, duplicates, unhelpful booleans
combined$countycd <- NULL
combined$countynm <- NULL
combined$locale2 <- NULL
combined$`institution name` <- NULL
combined$opeid <- NULL
combined$opeid6 <- NULL
combined$city <- NULL
combined$grrtas <- NULL
combined$grrtnh <- NULL
combined$efug <- NULL
combined$pcuenrbk <- NULL
combined$pcuenrw <- NULL
combined$pcuenrwh <- NULL
combined$pcuenrhs <- NULL
combined$aanapii <- NULL
combined$annhi <- NULL
combined$tribal <- NULL
combined$hsi <- NULL
combined$nanti <- NULL
combined$menonly <- NULL
combined$womenonly <- NULL
combined$pbi <- NULL
combined$hbcu <- NULL
combined[,c("satnum", "satpct", "actnum", "actpct", "satvr25", "satvr75", "satmt25", "satmt75", "actcm25", "actcm75", "acten25", "acten75", "actmt25", "actmt75")] <- NULL
combined$cdr2 <- NULL
combined$cdr3 <- NULL
combined$loan_ever <- NULL
combined$par_ed_n <- NULL
combined$appl_sch_n <- NULL

#change to decimal
combined[, c("dvadm02", "dvadm03", "dvadm05", "dvadm06", "dvef14", "floan_p", "loan_p")] <- combined[, c("dvadm02", "dvadm03", "dvadm05", "dvadm06", "dvef14", "floan_p", "loan_p")]/100
combined[,c("grrt2m", "grrtan", "grrtap", "grrtbk", "grrths", "grrtm", "grrtun", "grrtw", "grrtwh")] <- combined[,c("grrt2m", "grrtan", "grrtap", "grrtbk", "grrths", "grrtm", "grrtun", "grrtw", "grrtwh")]/100

#select out repayment
combined <- combined %>% select(-contains("rpy"))

#not using these anymore
combined[,c("npt412", "npt422", "npt432", "npt442", "npt452")] <- NULL

#select columns with <= 50% NA
combined <- combined[, which((colSums(is.na(combined)) / nrow(combined)) <= 0.5)]
```

To CSV
```{r}
write_csv(testScores, '~/Desktop/testScores.csv')
write_csv(repaymentRate, '~/Desktop/repayment.csv')
write_csv(combined, '~/Desktop/combined.csv')
```


-----


# Dimensionality Reduction (Kmeans on PCIP)
```{r}
#kmeans cluster analysis on pcip
combined %>% select(starts_with("pcip"), "unitid", 'year') -> pcip

#filter out empty rows
pcip <- pcip[rowSums(is.na(pcip)) != max(rowSums(is.na(pcip))), ]

#kmeans - 4clusters
p <- pcip %>% select(-c("unitid", 'year')) %>% scale()
k4 <- kmeans(p, centers = 4, iter.max = 25, nstart = 25)
#fviz_cluster(k4, data = p)

#check table of clusters
table(k4$cluster)

#add cluster assignment and drop all pcip cols
pcip$cluster <- k4$cluster
pcip1 <- pcip %>% select(unitid, year, cluster)

# merge to combined
df_k4cluster <- inner_join(combined, pcip1, by = c("unitid", "year") )
```

# PCA on PCIP
```{r}
## generate the correlation matrix
pcip %>% 
  select(-unitid, -cluster) -> pcip2

pcip_cor <- cor(pcip2)

corrplot(pcip_cor, 
         method = "color", 
         type="upper", 
         diag=F,
         order = "hclust")
```

Learned that PCIP only really explain about 12% of the variance in the data, maybe let's not use them or rely on them heavily.
```{r}
# pca
pcip_pca = prcomp(pcip2, center=T, scale=T)
fviz_screeplot(pcip_pca, addlables = T, ylim=c(0, 20))
get_eigenvalue(pcip_pca)

#visualize
fviz_pca_var(pcip_pca, col.var = "contrib")
fviz_contrib(pcip_pca, choice = "var")
fviz_contrib(pcip_pca, choice = "var", axes = 2)
fviz_contrib(pcip_pca, choice = "var", axes = 3)
```


------


# Imputation - create various copies of `combined` using different imputation methods

Impute column-wise median values grouped by year
```{r}
impute_by_year <- function(df, statistic){
  
  #select only numeric columns
  numeric <- df %>% select_if(is.numeric)
  
  #calculate col-wise medians grouped by the year
  tmp <- numeric %>%
    group_by(year) %>%
    summarise_at(vars(-unitid), funs(statistic(., na.rm=TRUE)))
  
  #pivot those bad boys
  numeric_long <- pivot_longer(numeric, 3:tail(names(df),1))
  tmp_long <- pivot_longer(tmp, 2:tail(names(df),1))
  
  #join by identifying year and name
  joined <- left_join(numeric_long, tmp_long, by = c("year", "name"))
  
  #keep only filled value
  joined$lyufan <- ifelse(is.na(joined$value.x) == T, joined$value.y, joined$value.x)
  
  #get rid of the old data
  joined$value.x <- NULL
  joined$value.y <- NULL
  
  #new df!
  output <- pivot_wider(joined, names_from = name, values_from = lyufan)
  
  #return
  return(output)
}
```


------


# Df Versions
## 1. Median imputation by year without cluster
```{r}
#mean
df_mean <- copy(combined)
df_mean <- impute_by_year(df_mean, mean)
df_mean <- df_mean[, colSums(is.na(df_mean)) <= 10000] #get rid of anything left that has over 10k NAs
df_mean[is.na(df_mean)] <- 0 #empty out remaining NAs

#median
df_median <- copy(combined)
df_median <- impute_by_year(df_median, median)
df_median <- df_median[, colSums(is.na(df_median)) <= 10000] #get rid of anything left that has over 10k NAs
df_median[is.na(df_median)] <- 0 #empty out remaining NAs

#min
df_min <- copy(combined)
df_min <- impute_by_year(df_min, min)
df_min <- df_min[, colSums(is.na(df_min)) <= 10000] #get rid of anything left that has over 10k NAs
df_min[is.na(df_min)] <- 0 #empty out remaining NAs

#max
df_max <- copy(combined)
df_max <- impute_by_year(df_max, max)
df_max <- df_max[, colSums(is.na(df_max)) <= 10000] #get rid of anything left that has over 10k NAs
df_max[is.na(df_max)] <- 0 #empty out remaining NAs
```


## 2. Median imputation by year with cluster
```{r}
#mean
df_mean_c <- copy(df_k4cluster)
df_mean_c <- impute_by_year(df_mean_c, mean)
df_mean_c <- df_mean_c[, colSums(is.na(df_mean_c)) <= 10000] #get rid of anything left that has over 10k NAs
df_mean_c[is.na(df_mean_c)] <- 0 #empty out remaining NAs

#median
df_median_c <- copy(df_k4cluster)
df_median_c <- impute_by_year(df_median_c, median)
df_median_c <- df_median_c[, colSums(is.na(df_median_c)) <= 10000] #get rid of anything left that has over 10k NAs
df_median_c[is.na(df_median_c)] <- 0 #empty out remaining NAs

#min
df_min_c <- copy(df_k4cluster)
df_min_c <- impute_by_year(df_min_c, min)
df_min_c <- df_min_c[, colSums(is.na(df_min_c)) <= 10000] #get rid of anything left that has over 10k NAs
df_min_c[is.na(df_min_c)] <- 0 #empty out remaining NAs

#max
df_max_c <- copy(df_k4cluster)
df_max_c <- impute_by_year(df_max_c, max)
df_max_c <- df_max_c[, colSums(is.na(df_max_c)) <= 10000] #get rid of anything left that has over 10k NAs
df_max_c[is.na(df_max_c)] <- 0 #empty out remaining NAs
```


3. Replace all NA values in original df with 0 (uni didn't report them or keep track scenario)
```{r}
#school didn't report it/change default NAs to 0
combined_zeroes <- copy(combined)
combined_zeroes[is.na(combined_zeroes)] <- 0
```


4. Variable Specific
```{r warning=FALSE}
#create a copy
combined_variable <- copy(combined)

#replace booleans with probability of mean
combined_variable$pcuenran[is.na(combined_variable$pcuenran)] <- rbinom(n=length(combined_variable$pcuenran), size=1, prob=mean(combined_variable$pcuenran, na.rm = T))
combined_variable$pcuenrnh[is.na(combined_variable$pcuenrnh)] <- rbinom(n=length(combined_variable$pcuenrnh), size=1, prob=mean(combined_variable$pcuenrnh, na.rm = T))

#default NA to zeroes
combined_variable$default[is.na(combined_variable$default)] <- 0

#replace all NA in PCIP to 0
combined_variable[ ,grep("pcip", names(combined_variable))][is.na(combined_variable[ ,grep("pcip", names(combined_variable))])] <- 0

#no age comparison needed
combined_variable$dvef14 <- NULL

#median imputation
combined_variable %>%
  select(unitid, year, avgfacsal, cinsoff, cotsoff, debt_mdn, floan_a, 
         floan_p, grad_debt_mdn, grrtap, grrtbk, grrths,grrtm, grrtwh, grrtw, 
         loan_a, loan_p, stufacr) %>%
  impute_by_year(median) -> tmp_medians

#join back to combined_variable
combined_variable %>% 
  select(-c(names(tmp_medians)), unitid, year) %>%
  left_join(tmp_medians, by = c("unitid", "year")) -> combined_variable_median


#mean imputation
combined_variable %>%
  select(unitid, year, avgfacsal, cinsoff, cotsoff, debt_mdn, floan_a, 
         floan_p, grad_debt_mdn, grrtap, grrtbk, grrths,grrtm, grrtwh, grrtw, 
         loan_a, loan_p, stufacr) %>%
  impute_by_year(mean) -> tmp_mean

#join back to combined_variable
combined_variable %>% 
  select(-c(names(tmp_mean)), unitid, year) %>%
  left_join(tmp_mean, by = c("unitid", "year")) -> combined_variable_mean

#write out
write_csv(combined_variable_median, '~/Desktop/combined_variable_median.csv')
write_csv(combined_variable_mean, '~/Desktop/combined_variable_mean.csv')
```


# Write all data to csv for GCP
```{r}
dfs = list(combined_variable_mean, 
           combined_variable_median,
           df_median, 
           df_mean,
           df_min,
           df_max,
           df_median_c, 
           df_mean_c,
           df_min_c,
           df_max_c,
           combined_zeroes)
dfs_names = c("combined_variable_mean", 
           "combined_variable_median",
           "df_median", 
           "df_mean",
           "df_min",
           "df_max",
           "df_median_c", 
           "df_mean_c",
           "df_min_c",
           "df_max_c",
           "combined_zeroes")

for (i in 1:length(dfs)){
  df <- dfs[[i]]
  write_csv(df, paste0('~/Desktop/DFs/', dfs_names[i], '.csv'))
}

```


------


# Function
Create a function that takes in a dataframe (some version of combined after imputing), runs a model (RF), and records out-of-sample MSE on a log. 
```{r}
rf <- function(df, trees = 1){
  
  set.seed(1234)
  
  #split into train and test
  split_index <- sample(nrow(df), nrow(df)*0.8)
  df_train <- df[split_index, ]
  df_test <- df[-split_index, ]
  
  print("Data split successfully")
  
  #get rid of unnecessary columns for rf
  cat_names <- names(df)
  cat_names <- cat_names[!cat_names %in% c("cdr2","cdr3","unitid", "zip","stabbr","instnm", "train", "default")]
  loopformula <- "default ~ 1"
  
  #create formula
  for(name in cat_names){
    loopformula <- paste(loopformula, "+", name, sep = '')
  }
  formula <- as.formula(loopformula)
  
  print("Formula made successfully")
  
  #Train/Test
  x_train <- model.matrix(formula, df_train)[,-1]
  y_train <- df_train$default
  
  x_test <- model.matrix(formula, df_test)[, -1]
  y_test <- df_test$default
  
  print("Model.matrix good")
  
  #RF fit
  fit_rf <- randomForest(formula,
                         df_train,
                         ntree=trees,
                         do.trace=T)
  print("tree works")
    
  #Plot RF fit
  varImpPlot(fit_rf)
  
  print("varimp yay")
  
  #Predict
  yhat_rf_train <- predict(fit_rf, df_train)
  mse_rf <- mean((yhat_rf_train -df_train$default) ^ 2)
  
  yhat_rf_test <- predict(fit_rf, df_test)
  mse_rf_test <- mean((yhat_rf_test - df_test$default) ^ 2)
  
  #results
  df_name = deparse(substitute(df))
  results = list(df_name, mse_rf, mse_rf_test)
  return(results)
}
```



# Test out rf function
```{r}
#initialize log
rf_results <- data.frame(name=numeric(),
                         mse_train=numeric(),
                         mse_test = numeric())

#for loop to run through df's
dfs = list(combined_variable_mean, 
           combined_variable_median,
           df_median, 
           df_mean,
           df_min,
           df_max,
           df_median_c, 
           df_mean_c,
           df_min_c,
           df_max_c,
           combined_zeroes)

dfs_names = c("combined_variable_mean", 
           "combined_variable_median",
           "df_median", 
           "df_mean",
           "df_min",
           "df_max",
           "df_median_c", 
           "df_mean_c",
           "df_min_c",
           "df_max_c",
           "combined_zeroes")

#loop
for (i in 1:length(dfs)){
  df <- dfs[[i]]
  result = rf(df, trees = 10)
  rf_results <- rf_results %>% add_row(name = dfs_names[i],
                                       mse_train = result[[2]],
                                       mse_test = result[[3]])
}
```

------

# ACT/SAT influence
```{r}


```


------





