---
title: "Untitled"
author: "Lyufan Pan"
date: "March 19, 2020"
output: html_document
---

# Student Loan Default Rate

-----

Load those libraries!!
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table)
library(corrplot)
library(factoextra)
library(readr)
library(matrixStats)
library(randomForest)
library(rapportools)
```

# Data cleaning
```{r message=FALSE, warning=FALSE}
#load
scorecard <- readRDS("~/Desktop/cleaned_scorecard.rds")
ipeds <- read_csv("~/Desktop/ipeds.csv")

#clean
names(scorecard) <- tolower(names(scorecard))
names(ipeds) <- tolower(names(ipeds))

#merge datasets based on unitid and year
combined <- merge(ipeds, scorecard, by = c("unitid", "year"))

#remove empty columns, duplicates, unhelpful booleans
combined$countycd <- NULL
combined$countynm <- NULL
combined$locale2 <- NULL
combined$`institution name` <- NULL
combined$opeid <- NULL
combined$opeid6 <- NULL
combined$city <- NULL
combined$grrtas <- NULL
combined$grrtnh <- NULL
combined$efug <- NULL
combined$pcuenrbk <- NULL
combined$pcuenrw <- NULL
combined$pcuenrwh <- NULL
combined$pcuenrhs <- NULL

#combine default rates
combined$default <- rowMeans(combined[,c("cdr2","cdr3")], na.rm = T)
```


-----


# Dimensionality Reduction (Kmeans on PCIP)
```{r}
#kmeans cluster analysis on pcip
combined %>% select(starts_with("pcip"), "unitid", 'year') -> pcip

#filter out empty rows
pcip <- pcip[rowSums(is.na(pcip)) != max(rowSums(is.na(pcip))), ]

#kmeans - 4clusters
p <- pcip %>% select(-c("unitid", 'year')) %>% scale()
k4 <- kmeans(p, centers = 4, iter.max = 25, nstart = 25)
#fviz_cluster(k4, data = p)

#check table of clusters
table(k4$cluster)

#add cluster assignment and drop all pcip cols
pcip$cluster <- k4$cluster
pcip1 <- pcip %>% select(unitid, year, cluster)

# merge to combined
df_k4cluster <- inner_join(combined, pcip1, by = c("unitid", "year") )
```

# PCA on PCIP
```{r}
## generate the correlation matrix
pcip %>% 
  select(-unitid, -cluster) -> pcip2

pcip_cor <- cor(pcip2)

corrplot(pcip_cor, 
         method = "color", 
         type="upper", 
         diag=F,
         order = "hclust")
```

Learned that PCIP only really explain about 12% of the variance in the data, maybe let's not use them or rely on them heavily.
```{r}
# pca
pcip_pca = prcomp(pcip2, center=T, scale=T)
fviz_screeplot(pcip_pca, addlables = T, ylim=c(0, 20))
get_eigenvalue(pcip_pca)

#visualize
fviz_pca_var(pcip_pca, col.var = "contrib")
fviz_contrib(pcip_pca, choice = "var")
fviz_contrib(pcip_pca, choice = "var", axes = 2)
fviz_contrib(pcip_pca, choice = "var", axes = 3)
```


------


# Experimenting with data loss based on `NA` removal
```{r}
# remove columns with less than X% NA
twenty <- combined[, which((colSums(is.na(combined)) / nrow(combined)) > 0.2) ]
fifty <- combined[, which((colSums(is.na(combined)) / nrow(combined)) > 0.5) ]
seventy <- combined[, which((colSums(is.na(combined)) / nrow(combined)) > 0.7) ]
```


------


# Imputation - create various copies of `combined` using different imputation methods

Impute column-wise median values grouped by year
```{r}
impute_by_year <- function(df, statistic){
  
  #select only numeric columns
  numeric <- df %>% select_if(is.numeric)
  
  #calculate col-wise medians grouped by the year
  tmp <- numeric %>%
    group_by(year) %>%
    summarise_at(vars(-unitid), funs(statistic(., na.rm=TRUE)))
  
  #pivot those bad boys
  numeric_long <- pivot_longer(numeric, npt412:tail(names(df),1))
  tmp_long <- pivot_longer(tmp, npt412:tail(names(df),1))
  
  #join by identifying year and name
  joined <- left_join(numeric_long, tmp_long, by = c("year", "name"))
  
  #keep only filled value
  joined$lyufan <- ifelse(is.na(joined$value.x) == T, joined$value.y, joined$value.x)
  
  #get rid of the old data
  joined$value.x <- NULL
  joined$value.y <- NULL
  
  #new df!
  output <- pivot_wider(joined, names_from = name, values_from = lyufan)
  
  #return
  return(output)
}

#check NAs
#colSums(is.na(df_medians))
```


------
MAX/Min/mean
variable specific NA replacement

unitid
year
logicals - 


# Df Versions
## 1. Median imputation by year without cluster
```{r}
#mean
df_mean <- impute_by_year(combined, mean)
df_mean <- df_mean[, colSums(is.na(df_mean)) < 10000] #get rid of anything left that has over 10k NAs
df_mean[is.na(df_mean)] <- 0 #empty out remaining NAs

#median
df_medians <- impute_by_year(combined, median)
df_medians <- df_medians[, colSums(is.na(df_medians)) < 10000] #get rid of anything left that has over 10k NAs
df_medians[is.na(df_medians)] <- 0 #empty out remaining NAs

#min
df_min <- impute_by_year(combined, min)
df_min <- df_min[, colSums(is.na(df_min)) < 10000] #get rid of anything left that has over 10k NAs
df_min[is.na(df_min)] <- 0 #empty out remaining NAs

#max
df_max <- impute_by_year(combined, max)
df_max <- df_max[, colSums(is.na(df_max)) < 10000] #get rid of anything left that has over 10k NAs
df_max[is.na(df_max)] <- 0 #empty out remaining NAs
```


CLEAN UP
2. Median imputation by year with cluster
```{r}
#no cluster
df_medians_c <- impute_by_year(df_k4cluster, median)

#get rid of anything left that has over 10k NAs
df_medians_c <- df_medians_c[, colSums(is.na(df_medians_c)) < 10000]

#empty out remaining NAs
df_medians_c[is.na(df_medians_c)] <- 0
```

3. Replace all NA values in original df with 0 (uni didn't report them or keep track scenario)
```{r}
#school didn't report it/change default NAs to 0
combined_zeroes <- combined
combined_zeroes[is.na(combined_zeroes)] <- 0
```

4. Variable Specific
```{r}
#only for the boolean values - replace with the column specific means
combined_variable <- copy(combined)
lst <- c("pcuenr2m", "pcuenran", "pcuenras", "pcuenrnh", "pcuenrun")



```




------



# Function
Create a function that takes in a dataframe (some version of combined after imputing), runs a model (RF), and records out-of-sample MSE on a log. 
```{r}
rf <- function(df){
  
  set.seed(1234)
    
  #split into train and test
  split_index <- sample(nrow(df), nrow(df)*0.8)
  df_train <- df[split_index, ]
  df_test <- df[-split_index, ]
  
  print("Data split successfully")
  
  #get rid of unnecessary columns for rf
  cat_names <- names(df)
  cat_names <- cat_names[!cat_names %in% c("cdr2","cdr3","unitid", "zip","stabbr","instnm", "train", "default")]
  loopformula <- "default ~ 1"
  
  #create formula
  for(name in cat_names){
    loopformula <- paste(loopformula, "+", name, sep = '')
  }
  formula <- as.formula(loopformula)
  
  print("Formula made successfully")
  
  #Train/Test
  x_train <- model.matrix(formula, df_train)[,-1]
  y_train <- df_train$default
  
  x_test <- model.matrix(formula, df_test)[, -1]
  y_test <- df_test$default
  
  print("Model.matrix good")
  
  #RF fit
  fit_rf <- randomForest(formula,
                         df_train,
                         ntree=5,
                         do.trace=T)
  
  print("tree works")
    
  #Plot RF fit
  varImpPlot(fit_rf)
  
  print("varimp yay")
  
  #Predict
  yhat_rf_train <- predict(fit_rf, df_train)
  mse_rf <- mean((yhat_rf_train -df_train$default) ^ 2)
  
  yhat_rf_test <- predict(fit_rf, df_test)
  mse_rf_test <- mean((yhat_rf_test - df_test$default) ^ 2)
  
  #results
  df_name = deparse(substitute(df))
  results = list(df_name, mse_rf, mse_rf_test)
  return(results)
}
```



# Test out rf function
```{r}
#initialize log
rf_results <- tibble(name = "name", mse_train = 'mse_train', mse_test = "mse_test")

#for loop to run through df's
dfs = list(df_medians, df_medians_c, combined_zeroes) #add more when ready

#loop
for (i in 1:length(dfs)){
  result = rf(dfs[i])
  rf_results %>% add_row(name = result[1],
                         mse_train = result[2],
                         mse_test = result[3])
}
```


------

# Segementation between cohorts and schools
```{r}
# mansi
```


------





